\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{siunitx}
\sisetup{retain-explicit-plus=true}
\usepackage{float}
\usepackage[section]{placeins}
% pgfplots not needed after switching to tables

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{textgreek}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

\title{Personality Manipulation in Large Language Models: A Systematic Probe into Behavioral Representation and Control Mechanisms}

\author{Author Name 1 \\
  Institution \\
  \texttt{email@domain.com} \\\And
  Author Name 2 \\
  Institution \\
  \texttt{email@domain.com} \\}

\begin{document}
\maketitle
\begin{abstract}
While personality design is common in large language models, the mechanisms and downstream effects remain poorly understood. This paper uses personality manipulation as a probe to understand how behavioral traits are encoded and controlled in LLMs through three approaches: in-context learning (ICL), parameter-efficient fine-tuning (PEFT), and activation-based steering vectors. We develop an framework to validate Big Five personality trait induction and measure its impact on bias and task performance. Our study evaluates ICL, LoRA-style PEFT, and activation steering vectors derived from layer-wise activation differences on Gemma-2-2B-IT and LLaMA-3-8B-Instruct. Results show that ICL generates large immediate trait shifts, PEFT produces stable personality changes, and activation steering achieves effective performance with low computational cost. Downstream evaluation on BBQ, MMLU, and GAIA benchmarks reveals distinct trade-offs between personality control strength and task performance across methods. Our manipulation experiments reveal distinct mechanisms of personality control and establish activation-based steering as an interpretable view of personality representation, providing practical guidance and insights into behavioral control mechanisms in LLMs.
\end{abstract}

\input{sections/01_introduction}
\input{sections/03_methodology}
\input{sections/05_results}
\input{sections/06_discussion}

% Appendices
\appendix
\input{appendices/appendix_A_background}
\input{appendices/appendix_B_prompting}
\input{appendices/appendix_C_peft}
\input{appendices/appendix_D_activation_steering}
\input{appendices/appendix_E_experimental_design}
\input{appendices/appendix_F_trait_results}
\input{appendices/appendix_J_alignment_results}
\input{appendices/appendix_G_downstream_analysis}
\input{appendices/appendix_H_comparative_analysis}
\input{appendices/appendix_I_discussion_extended}
\input{appendices/appendix_K_benchmarks}

% Bibliography entries
\bibliography{references}

\end{document}