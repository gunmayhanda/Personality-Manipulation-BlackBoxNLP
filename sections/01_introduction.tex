\section{Introduction}

Designing and controlling personality in large language models (LLMs) is increasingly common, yet the trade-offs between personality control and task capability are poorly quantified. Prior work spans in-context learning (ICL) conditioning, parameter-efficient fine-tuning (PEFT; e.g., LoRA), and activation-based steering of internal representations, alongside evaluation frameworks for knowledge (MMLU), reasoning (GAIA), and bias (BBQ). However, consistent comparisons across methods and traits remain limited. From an interpretability perspective, personality manipulation provides a powerful experimental paradigm for understanding how behavioral traits are encoded in neural representations. By systematically applying different manipulation techniques and measuring their effects, we can reverse-engineer the representational structures underlying personality expression. Each manipulation approach—ICL, parameter modification, and activation intervention—serves as a distinct probe into different layers of personality representation, revealing complementary insights into the mechanisms underlying behavioral control in LLMs. We study three families of approaches for Big Five personality manipulation: ICL, PEFT (LoRA adapters), and activation steering. To ensure fair comparison despite run-to-run baseline differences, we adopt a relative change (\(\Delta\)) analysis: all effects are measured within each method's own baseline run. We complement benchmark outcomes with an independent alignment validation task that directly measures how strongly a target personality is expressed. Our contributions are threefold: (1) a unified, \(\Delta\)-based evaluation framework that reveals method-specific personality encoding mechanisms independent of absolute baselines; (2) a cross-method, cross-trait analysis that uncovers trait-specific representational structures by linking \(\Delta\) capability changes to independent alignment strength; and (3) mechanistic insights into personality control pathways that provide both practical guidance and fundamental understanding of behavioral manipulation in neural language models, with implementation details and extended results in the appendices.