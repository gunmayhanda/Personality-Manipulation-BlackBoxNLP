\section{Methodology Overview}

We employ a controlled three-experiment design evaluating personality manipulation across prompting, PEFT, and activation steering methods. All approaches target Big Five personality traits (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) using the Holistic AI Personality Classifier for standardized validation.

\textbf{Method 1 - Prompting:} We implement role-playing prompts across all three models (Gemma-2, LLaMA-3, GPT-4.1) using the template "You are an expert assistant who embodies the personality trait of {personality}. Your task is to solve the following problem." This enables bidirectional manipulation assessment with high and low trait conditions.

\textbf{Method 2 - PEFT:} We apply LoRA fine-tuning to both Gemma-2 and LLaMA-3 models using the Holistic AI personality manipulation dataset. For LLaMA-3, we use r=64, alpha=16, dropout=0.1, targeting q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj, and down\_proj modules. Training runs for 2 epochs with batch size 2, learning rate 2e-4, and cosine learning rate scheduling.

\textbf{Method 3 - Activation Steering:} We implement layer-wise activation modification for Gemma-2 using contrastive data between high and low trait responses. Target layers [5, 10, 15, 20] focus on mid-to-late transformer layers. Steering vectors are extracted from post-attention layer norm activations and calibrated using linear search across strength values. Final parameters: Extraversion (Layer 15, Strength 200.0), Agreeableness (Layer 10, Strength 100.0), Neuroticism (Layer 15, Strength 200.0), Openness (Layer 15, Strength 110.0), Conscientiousness (Layer 15, Strength 250.0).

All methods are evaluated on MMLU, BBQ, and GAIA benchmarks to assess downstream performance impacts. The experimental design enables direct comparison of manipulation effectiveness, stability, and computational efficiency across approaches.
