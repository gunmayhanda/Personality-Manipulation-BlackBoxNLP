\section{In-Context Learning Methodology and Results}
\label{app:icl}

\subsection{ICL Setup and Templates}

For ICL-based personality manipulation, we employ role-playing templates across two separate models (Gemma-2, LLaMA-3). Our ICL strategy follows a role-playing approach, where the model is instructed to adopt specific personality characteristics through the template: "You are an expert assistant who embodies the personality trait of {personality}. Your task is to solve the following problem."

We construct prompts that explicitly target each Big Five trait, using both positive and negative trait descriptions to enable bidirectional manipulation. The template approach enables consistent personality conditioning across different model architectures.

Each personality condition is evaluated using MMLU benchmark questions, ensuring that trait measurement occurs on content distinct from the conditioning prompts.

\subsection{Experimental Configuration}

- Models: Gemma-2-2B-IT and LLaMA-3-8B-Instruct
- Temperature: 0.7 for personality expression
- Max tokens: 100 per response
- Evaluation: MMLU benchmark across 7 strategic subjects
- Baseline measurement: Neutral ICL without personality conditioning

\subsection{ICL Results (\(\Delta\)-based)}

ICL effects are reported as within-run \(\Delta\) relative to the method's Baseline. On Gemma-2:
\begin{itemize}
\item MMLU (Accuracy\_Avg): modest negative \(\Delta\) across traits relative to Baseline.
\item GAIA (Accuracy): small positive \(\Delta\) on average.
\item BBQ (\(S_{AMB}\)): small trait-dependent shifts.
\end{itemize}

Independent alignment validation shows strong alignment for most traits (e.g., Gemma extraversion 1.00, neuroticism 1.00; openness high), with agreeableness comparatively lower.

\subsubsection{Computational Requirements}

ICL requires minimal computational overhead due to:
- No parameter updates or fine-tuning
- Immediate personality induction
- Consistent performance across traits
- No additional training data requirements
